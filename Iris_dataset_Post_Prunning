from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import plot_tree

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a decision tree classifier with pre-pruning parameters
dt = DecisionTreeClassifier(criterion='entropy', random_state=42)

# Fit the decision tree classifier to the training data
dt.fit(X_train, y_train)

# Evaluate the model on the testing data
y_pred = dt.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}".format(accuracy))

# Pruning the decision tree using cost complexity pruning
path = dt.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas[:-1]  # Remove the last alpha as it corresponds to a fully grown tree

# Create an array to store the decision tree models
dt_models = []
for ccp_alpha in ccp_alphas:
    dt_model = DecisionTreeClassifier(criterion='entropy', random_state=42, ccp_alpha=ccp_alpha)
    dt_model.fit(X_train, y_train)
    dt_models.append(dt_model)

# Find the optimal alpha based on cross-validation accuracy
acc_scores = [accuracy_score(y_test, dt_model.predict(X_test)) for dt_model in dt_models]
best_alpha = ccp_alphas[acc_scores.index(max(acc_scores))]

# Create the final decision tree model with optimal alpha
final_dt = DecisionTreeClassifier(criterion='entropy', random_state=42, ccp_alpha=best_alpha)
final_dt.fit(X_train, y_train)

# Evaluate the final model on the testing data
y_pred = final_dt.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy after post-pruning: {:.2f}".format(accuracy))

# Plot the final decision tree
plot_tree(final_dt, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
